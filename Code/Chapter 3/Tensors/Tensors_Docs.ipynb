{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Tensor from Docs\n",
    "\n",
    ">A __torch.Tensor__ is a multi-dimensional matrix containing elements of a single data type.\n",
    "\n",
    "[Tensor Docs](https://pytorch.org/docs/stable/tensors.html \"Go and learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Creation\n",
    "\n",
    "[Creation Ops](https://pytorch.org/docs/stable/torch.html#tensor-creation-ops \"Learn to create first\")\n",
    "\n",
    "1. To create a tensor with pre-existing data, use ```torch.tensor()```.\n",
    "\n",
    "2. To create a tensor with specific size, use ```torch.*```.\n",
    "\n",
    "3. To create a tensor with the same size (and similar types) as another tensor, use ```torch.*_like```.\n",
    "\n",
    "4. To create a tensor with similar type but different size as another tensor, use ```tensor.new_*```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.tensor.item() to get python number \n",
    "print(x[1][2])\n",
    "\n",
    "x[1][2].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Views\n",
    "\n",
    "PyTorch allows a tensor to be a View of an existing tensor. View tensor shares the same underlying data with its base tensor. Supporting View avoids explicit data copy, thus allows us to do fast and memory efficient reshaping, slicing and element-wise operations.\n",
    "\n",
    "<p style=\"color:red\">Taking a view of contiguous tensor could potentially produce a non-contiguous tensor. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = torch.tensor([[0, 1],[2, 3]])\n",
    "base.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = base.transpose(0, 1)  \n",
    "# `t` is a view of `base`. No data movement happened here.\n",
    "# View tensors might be non-contiguous.\n",
    "t.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a contiguous tensor, call `.contiguous()` to enforce\n",
    "# copying data when `t` is not contiguous.\n",
    "c = t.contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new tensor from existing\n",
    "\n",
    "1. **new_tensor**: Returns a new Tensor with ```data``` as the tensor data. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.\n",
    "<p style=\"color:red\">If you have a numpy array and want to avoid a copy, use torch.from_numpy().</p><br>\n",
    "2. **new_full**: Returns a Tensor of size ```size``` filled with fill_value.\n",
    "3. **new_empty**: Returns a Tensor of size size filled with uninitialized data.\n",
    "4. **new_ones**: Returns a Tensor of size size filled with 1\n",
    "5. **new_zeros**: Returns a Tensor of size size filled with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test tensor\n",
    "tensor = torch.tensor((2,2), dtype=torch.float32)\n",
    "\n",
    "# 1 new_tensor\n",
    "data = [[0, 1], [2, 3]]\n",
    "tensor.new_tensor(data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1416, 3.1416, 3.1416, 3.1416],\n",
       "        [3.1416, 3.1416, 3.1416, 3.1416],\n",
       "        [3.1416, 3.1416, 3.1416, 3.1416]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. new_full\n",
    "tensor.new_full(size = (3, 4), fill_value = 3.141592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9941e-31,  4.5902e-41, -1.9941e-31],\n",
       "        [ 4.5902e-41,  0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. new_empty\n",
    "tensor.new_empty((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. new_ones \n",
    "# 5. new_zeros\n",
    "print(tensor.new_ones((2, 3)))\n",
    "tensor.new_zeros((2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computational Graph**\n",
    "\n",
    "```backward(gradient=None, retain_graph=None, create_graph=False)```\n",
    "\n",
    "Computes the gradient of current tensor w.r.t. graph leaves.\n",
    "\n",
    "The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([5.,5.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = x.pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 10.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.tensor.expand()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1], [2], [3]])\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [2, 2, 2, 2],\n",
       "        [3, 3, 3, 3]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.expand(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [2, 2, 2, 2],\n",
       "        [3, 3, 3, 3]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.expand(-1, 4)   \n",
    "# -1 means not changing the size of that dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**is_leaf**\n",
    "\n",
    "All Tensors that have ```requires_grad``` which is **False** will be leaf Tensors by convention.\n",
    "\n",
    "For Tensors that have ```requires_grad``` which is **True**, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so ```grad_fn``` is None.\n",
    "\n",
    "Only leaf Tensors will have their ```grad``` populated during a call to ```backward()```. To get grad populated for non-leaf Tensors, you can use ```retain_grad()```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    ">>> a = torch.rand(10, requires_grad=True)\n",
    ">>> a.is_leaf\n",
    "True\n",
    ">>> b = torch.rand(10, requires_grad=True).cuda()\n",
    ">>> b.is_leaf\n",
    "False\n",
    "# b was created by the operation that cast a cpu Tensor into a cuda Tensor\n",
    ">>> c = torch.rand(10, requires_grad=True) + 2\n",
    ">>> c.is_leaf\n",
    "False\n",
    "# c was created by the addition operation\n",
    ">>> d = torch.rand(10).cuda()\n",
    ">>> d.is_leaf\n",
    "True\n",
    "# d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)\n",
    ">>> e = torch.rand(10).cuda().requires_grad_()\n",
    ">>> e.is_leaf\n",
    "True\n",
    "# e requires gradients and has no operations creating it\n",
    ">>> f = torch.rand(10, requires_grad=True, device=\"cuda\")\n",
    ">>> f.is_leaf\n",
    "True\n",
    "# f requires grad, has no operation creating it\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.squeeze(input, dim=None, out=None)**\n",
    "\n",
    "Returns a tensor with all the dimensions of input of size 1 removed.\n",
    "\n",
    "For example, if input is of shape:(A×1×B×C×1×D) then the out tensor will be of shape: (A×B×C×D) .\n",
    "\n",
    "When dim is given, a squeeze operation is done only in the given dimension. If input is of shape: (A×1×B) , squeeze(input, 0) leaves the tensor unchanged, but squeeze(input, 1) will squeeze the tensor to the shape (A×B) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(2, 1, 2, 1, 2)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.squeeze().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.ones(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.unsqueeze(input, dim)**\n",
    "\n",
    "Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "\n",
    "The returned tensor shares the same underlying data with this tensor.\n",
    "\n",
    "A dim value within the range ```[-input.dim() - 1, input.dim() + 1)]``` can be used. Negative dim will correspond to unsqueeze() applied at ```dim = dim + input.dim() + 1.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = y.unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.]],\n",
       "\n",
       "        [[1., 1.]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 2])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
